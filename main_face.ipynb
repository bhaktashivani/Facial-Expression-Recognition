{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project: Facial Expression Recognition\n",
    "#### By Shivani Bhakta and Payal Singh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from facenet_pytorch import MTCNN\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import os.path as osp\n",
    "import cv2 \n",
    "import sys\n",
    "\n",
    "from woodnet import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import copy\n",
    "\n",
    "USE_GPU = True\n",
    "\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tufts Face DB \n",
    "In this project we use a dataset named Tufts Face Database for Facial Recognition. This dataset is made up with over 10,000 images of 74 female and 38 males from more than 15 countried with the age range between 4-70 years old. This dataset contains 7 image modalities, but we only use a subset of it, from the Tufts Face Databse 2D RGB Around (TDRGBA).\n",
    "\n",
    "\n",
    "In this project we use few pictures for each person to train the model and the rest to predict which person's picture it is. There are total of 50 people, whose pictures we are going to use. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the dataset\n",
    "#### Reduce the size of the dataset to 10x smaller and store the new images into dataset/resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# import PI\n",
    "\n",
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(24, 16))\n",
    "for person in range(1, 51): \n",
    "    if person == 47: continue\n",
    "    for idx in range(1,6):\n",
    "        t = 'dataset/' + str(person) + '/TD_RGB_E_{}.jpg'.format(idx)\n",
    "\n",
    "        im = Image.open(t)\n",
    "        width, height = im.size #3072,4608\n",
    "\n",
    "        newsize = (width//10, height//10)\n",
    "        im2 = im.resize(newsize) # resized Image Size:  (460, 307)\n",
    "    \n",
    "        fn = 'TD_RGB_E_{}_{}.jpg'.format(person,idx)\n",
    "        im2.save('dataset/resized/' + fn)\n",
    "    \n",
    "        if person == 1: \n",
    "            # Display the first person's all images\n",
    "            if idx in [1,2]: \n",
    "                i = 0\n",
    "                j = idx - 1\n",
    "\n",
    "            elif idx in [3,4]:\n",
    "                i = 1\n",
    "                j = idx - 3    \n",
    "            else: \n",
    "                i = 2\n",
    "                j = idx - 5 \n",
    "            axes[i][j].axis('off')\n",
    "            axes[i][j].imshow(im2)\n",
    "            axes[i][j].set_title('Person 1 , image {}'.format(idx))\n",
    "#             print(\"resized Image Size: \", im2.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49, 3, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "def get_faces():\n",
    "    '''\n",
    "    input: \n",
    "        n: number of images to load from the folder. \n",
    "    output: \n",
    "        neutral_faces: numpy array with each column corresponds to a data point(each image of neutral faces) \n",
    "        smiling_faces: numpy array with each column corresponds to a data point(each image of smiling faces)\n",
    "    '''\n",
    "\n",
    "    n = 49    # will change when do data augmentation\n",
    "#     neutral_faces = np.zeros((n,224,224,3))\n",
    "#     smiling_faces = np.zeros((n,224,224,3))\n",
    "#     eyesclosed_faces = np.zeros((n,224,224,3))\n",
    "#     surprised_faces = np.zeros((n,224,224,3))\n",
    "#     sunglasses_faces = np.zeros((n,224,224,3))\n",
    "\n",
    "    neutral_faces = np.zeros((n,3,224,224))\n",
    "    smiling_faces = np.zeros((n,3,224,224))\n",
    "    eyesclosed_faces = np.zeros((n,3,224,224))\n",
    "    surprised_faces = np.zeros((n,3,224,224))\n",
    "    sunglasses_faces = np.zeros((n,3,224,224))\n",
    "\n",
    "    for i in range(1,n+1):\n",
    "        if i == 47: continue     \n",
    "        path = str('cropped/')\n",
    "            \n",
    "        for j in range(1,6):\n",
    "            image_path = path + 'TD_RGB_E_{}_{}.jpg'.format(i,j)\n",
    "            p = Image.open(image_path)        \n",
    "#             size = p.size\n",
    "#             p = ImageOps.grayscale(p)\n",
    "            p = np.transpose(np.array(p))\n",
    "#             print(np.transpose(p).shape)\n",
    "            \n",
    "            if j == 1: neutral_faces[i-1,:] = p\n",
    "            elif j == 2: smiling_faces[i-1,:] = p\n",
    "            elif j ==3: eyesclosed_faces[i-1,:] = p\n",
    "            elif j == 4: surprised_faces[i-1,:] = p\n",
    "            elif j == 5: sunglasses_faces[i-1,:] = p\n",
    "\n",
    "#     return np.transpose(neutral_faces), np.transpose(smiling_faces), np.transpose(eyesclosed_faces), np.transpose(surprised_faces), np.transpose(sunglasses_faces)\n",
    "    return neutral_faces, smiling_faces, eyesclosed_faces, surprised_faces, sunglasses_faces\n",
    "\n",
    "neutral_faces, smiling_faces, eyesclosed_faces, surprised_faces, sunglasses_faces = get_faces()\n",
    "print(neutral_faces.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(245, 3, 224, 224)\n",
      "<class 'numpy.ndarray'>\n",
      "(217, 3, 224, 224)\n",
      "image size:  (1, 3, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "# dataset\n",
    "\n",
    "neutral_faces, smiling_faces, eyesclosed_faces, surprised_faces, sunglasses_faces = get_faces()\n",
    "\n",
    "\n",
    "data = np.vstack((neutral_faces, smiling_faces, eyesclosed_faces, surprised_faces, sunglasses_faces))\n",
    "labels_0 = [0] * len(neutral_faces)\n",
    "labels_1 = [1] * len(neutral_faces)\n",
    "labels_2 = [2] * len(neutral_faces)\n",
    "labels_3 = [3] * len(neutral_faces)\n",
    "labels_4 = [4] * len(neutral_faces)\n",
    "\n",
    "labels = labels_0 + labels_1 + labels_2 + labels_3 + labels_4\n",
    "labels = np.array(labels)\n",
    "idxs = list(range(245))   # will change when do data augmentation\n",
    "np.random.shuffle(idxs)\n",
    "\n",
    "data = data[idxs]\n",
    "labels = labels[idxs]\n",
    "\n",
    "print(type(data))\n",
    "print(data.shape)\n",
    "\n",
    "test_train_split = 0.88889\n",
    "train_data = data[:int(test_train_split * len(idxs))]\n",
    "train_labels = labels[:int(test_train_split * len(idxs))]\n",
    "\n",
    "print(type(train_data))\n",
    "print(train_data.shape)\n",
    "\n",
    "test_data = data[int(test_train_split * len(idxs)):]\n",
    "test_labels = labels[int(test_train_split * len(idxs)):]\n",
    "image = test_data[0]\n",
    "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "print(\"image size: \", image.shape)\n",
    "# train_data = torch.tensor(train_data)\n",
    "# train_labels = torch.tensor(train_labels)\n",
    "# train_data = TensorDataset(train_data, train_labels)\n",
    "\n",
    "# test_data = torch.tensor(test_data)\n",
    "# test_labels = torch.tensor(test_labels)\n",
    "# test_data = TensorDataset(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(neutral_faces, smiling_faces, eyesclosed_faces, surprised_faces, sunglasses_faces, n): \n",
    "# def test(neutral_faces,smiling_faces,eyesclosed_faces, n): \n",
    "    '''\n",
    "    Function to test that I am able to recontruct the original image with after all the reshaping and shaping.\n",
    "    Checking the dimensions and shape as well. \n",
    "    '''\n",
    "    print(neutral_faces.shape)\n",
    "\n",
    "    plt.subplot(1,5,1)\n",
    "    plt.imshow((neutral_faces[:,n]).reshape((224,224)))\n",
    "    plt.subplot(1,5,2)\n",
    "    plt.imshow((smiling_faces[:,n]).reshape((224,224)))\n",
    "    plt.subplot(1,5,3)\n",
    "    plt.imshow((eyesclosed_faces[:,n]).reshape((224,224)))\n",
    "    plt.subplot(1,5,4)\n",
    "    plt.imshow((surprised_faces[:,n]).reshape((224,224)))\n",
    "    plt.subplot(1,5,5)\n",
    "    plt.imshow((sunglasses_faces[:,n]).reshape((224,224)))\n",
    "\n",
    "    # Executing test function        \n",
    "test(neutral_faces, smiling_faces, eyesclosed_faces, surprised_faces, sunglasses_faces, 25)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, targets, transform=None):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.targets[index]\n",
    "        \n",
    "        if self.transform:\n",
    "            x = Image.fromarray(self.data[index].astype(np.uint8).transpose(1,2,0))\n",
    "            x = self.transform(x)\n",
    "#         print(x)\n",
    "        data = {'x': x, 'y': y}\n",
    "        return data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "trans = [T.ToTensor(), T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]\n",
    "train_data = MyDataset(train_data, train_labels,trans)\n",
    "test_data = MyDataset(test_data, test_labels,trans)\n",
    "# dataloader = DataLoader(dataset, batch_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5/27 to do next:\n",
    "- need dataloader\n",
    "- figure out if data is already centered (if not, how to do it for PCA)\n",
    "- feature extraction (PCA)\n",
    "- train our ELMAN RNN (link for how to use RNN with Pytorch: https://www.kaggle.com/kanncaa1/recurrent-neural-network-with-pytorch, https://pytorch.org/docs/stable/generated/torch.nn.RNN.html)\n",
    "- find optimal weights\n",
    "- test\n",
    "- try out other types of netral networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference*\n",
    "Any publication using this database must reference to this: \n",
    "- Website: http://tdface.ece.tufts.edu/ and this \n",
    "- Paper: Panetta, Karen, Qianwen Wan, Sos Agaian, Srijith Rajeev, Shreyas Kamath, Rahul Rajendran, Shishir Rao et al. \"A comprehensive database for benchmarking imaging systems.\" IEEE Transactions on Pattern Analysis and Machine Intelligence (2018)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the resized data and save into cropped folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/timesler/guide-to-mtcnn-in-facenet-pytorch\n",
    "mtcnn = MTCNN(image_size=224, margin=20, keep_all=True, post_process=False, device='cuda')\n",
    "\n",
    "# face detection code, save cropped face \n",
    "# https://www.kaggle.com/timesler/guide-to-mtcnn-in-facenet-pytorch\n",
    "IMAGE_SIZE = 224\n",
    "\n",
    "for person in range(1, 51):\n",
    "    if person == 47: continue\n",
    "    for idx in range(1, 6):\n",
    "        # Load a single image and display\n",
    "        frame = cv2.imread(f'dataset/{person}/TD_RGB_E_{idx}.jpg')\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = cv2.resize(frame, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "        frame = Image.fromarray(frame)\n",
    "\n",
    "        mtcnn(frame, save_path=f'cropped/TD_RGB_E_{person}_{idx}.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAIN = len(train_data)\n",
    "NUM_TEST = len(test_data)\n",
    "\n",
    "def check_accuracy(loader, model):\n",
    "#     if loader.dataset.train:\n",
    "#         print('Checking accuracy on validation set')\n",
    "#     else:\n",
    "#         print('Checking accuracy on test set')   \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            x = data['x'].to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = data['y'].to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(model, optimizer, epochs=1):\n",
    "    \"\"\"\n",
    "    Train a model lol\n",
    "    \n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    model.train()  # put model to training mode\n",
    "    for e in range(epochs):\n",
    "        for t, data in enumerate(loader_train):\n",
    "#             print(data)\n",
    "            x = data['x'].to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = data['y'].to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "            if t % print_every == 0:\n",
    "                print('Epoch %d, Iteration %d, loss = %.4f' % (e, t, loss.item()))\n",
    "#                 check_accuracy(loader_val, model)\n",
    "#                 print()\n",
    "\n",
    "# We set up a Dataset object for each split (train / val / test); Datasets load\n",
    "# training examples one at a time, so we wrap each Dataset in a DataLoader which\n",
    "# iterates through the Dataset and forms minibatches. We divide the CIFAR-100\n",
    "# training set into train and val sets by passing a Sampler object to the\n",
    "# DataLoader telling how it should sample from the underlying Dataset.\n",
    "loader_train = DataLoader(train_data, batch_size=12, sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "\n",
    "# loader_val = DataLoader(cifar100_val, batch_size=batch_size, num_workers=2, \n",
    "#                         sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n",
    "\n",
    "# cifar100_test = dset.CIFAR100('./datasets/cifar100', train=False, download=True, \n",
    "#                             transform=transform_test)\n",
    "loader_test = DataLoader(test_data, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.MyDataset at 0x7fb773ce89d0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /tmp/xdg-cache/torch/hub/pytorch_vision_v0.8.2\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'loader_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-dfc39cb14140>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m# Tensor of shape 1000, with confidence scores over Imagenet's 1000 classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loader_train' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "model = torch.hub.load('pytorch/vision:v0.8.2', 'alexnet', pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     train_data = train_data#.to('cuda')\n",
    "#     model.to('cuda')\n",
    "for t, data in enumerate(loader_train): \n",
    "    with torch.no_grad():\n",
    "        output = model()\n",
    "    # Tensor of shape 1000, with confidence scores over Imagenet's 1000 classes\n",
    "    print(output[0])\n",
    "    # The output has unnormalized scores. To get probabilities, you can run a softmax on it.\n",
    "    probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "    print(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchvision\n",
    "# vgg19 = torchvision.models.vgg19(pretrained=True)\n",
    "\n",
    "# vgg19.classifier[1] = nn.Conv2d(vgg19.classifier[1].in_channels,5,1,1)\n",
    "\n",
    "from torchvision import datasets, models, transforms\n",
    " \n",
    "squeezenet = models.squeezenet1_1(pretrained=True, progress=True)\n",
    "# Replace the last layer with one with the correct number of channels\n",
    "network.load_state_dict\n",
    "num_ftr = squeezenet.classifier[1].in_channels\n",
    "squeezenet.classifier[1] = nn.Conv2d(num_ftr, 5, 1, 1)\n",
    "# squeezenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "model = VGG16()\n",
    "\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "# prepare the image for the VGG model\n",
    "image = preprocess_input(image)\n",
    "\n",
    "# predict the probability across all output classes\n",
    "yhat = model.predict(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define neural network\n",
    "model = WoodNet()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001, weight_decay=0.00001)\n",
    "\n",
    "print_every = 10\n",
    "train_model(model, optimizer, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set accuracy\n",
    "check_accuracy(loader_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
