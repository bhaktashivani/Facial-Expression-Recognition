{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project: Facial Expression Recognition\n",
    "#### By Shivani Bhakta and Payal Singh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from facenet_pytorch import MTCNN\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import os.path as osp\n",
    "import cv2 \n",
    "import sys\n",
    "\n",
    "from woodnet import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import copy\n",
    "\n",
    "USE_GPU = True\n",
    "\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tufts Face DB \n",
    "In this project we use a dataset named Tufts Face Database for Facial Recognition. This dataset is made up with over 10,000 images of 74 female and 38 males from more than 15 countried with the age range between 4-70 years old. This dataset contains 7 image modalities, but we only use a subset of it, from the Tufts Face Databse 2D RGB Around (TDRGBA).\n",
    "\n",
    "\n",
    "In this project we use few pictures for each person to train the model and the rest to predict which person's picture it is. There are total of 50 people, whose pictures we are going to use. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the dataset\n",
    "#### Reduce the size of the dataset to 10x smaller and store the new images into dataset/resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# import PI\n",
    "\n",
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(24, 16))\n",
    "for person in range(1, 51): \n",
    "    if person == 47: continue\n",
    "    for idx in range(1,6):\n",
    "        t = 'dataset/' + str(person) + '/TD_RGB_E_{}.jpg'.format(idx)\n",
    "\n",
    "        im = Image.open(t)\n",
    "        width, height = im.size #3072,4608\n",
    "\n",
    "        newsize = (width//10, height//10)\n",
    "        im2 = im.resize(newsize) # resized Image Size:  (460, 307)\n",
    "    \n",
    "        fn = 'TD_RGB_E_{}_{}.jpg'.format(person,idx)\n",
    "        im2.save('dataset/resized/' + fn)\n",
    "    \n",
    "        if person == 1: \n",
    "            # Display the first person's all images\n",
    "            if idx in [1,2]: \n",
    "                i = 0\n",
    "                j = idx - 1\n",
    "\n",
    "            elif idx in [3,4]:\n",
    "                i = 1\n",
    "                j = idx - 3    \n",
    "            else: \n",
    "                i = 2\n",
    "                j = idx - 5 \n",
    "            axes[i][j].axis('off')\n",
    "            axes[i][j].imshow(im2)\n",
    "            axes[i][j].set_title('Person 1 , image {}'.format(idx))\n",
    "#             print(\"resized Image Size: \", im2.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49, 3, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "def get_faces():\n",
    "    '''\n",
    "    input: \n",
    "        n: number of images to load from the folder. \n",
    "    output: \n",
    "        neutral_faces: numpy array with each column corresponds to a data point(each image of neutral faces) \n",
    "        smiling_faces: numpy array with each column corresponds to a data point(each image of smiling faces)\n",
    "    '''\n",
    "\n",
    "    n = 49    # will change when do data augmentation\n",
    "#     neutral_faces = np.zeros((n,224,224,3))\n",
    "#     smiling_faces = np.zeros((n,224,224,3))\n",
    "#     eyesclosed_faces = np.zeros((n,224,224,3))\n",
    "#     surprised_faces = np.zeros((n,224,224,3))\n",
    "#     sunglasses_faces = np.zeros((n,224,224,3))\n",
    "\n",
    "    neutral_faces = np.zeros((n,3,224,224))\n",
    "    smiling_faces = np.zeros((n,3,224,224))\n",
    "    eyesclosed_faces = np.zeros((n,3,224,224))\n",
    "    surprised_faces = np.zeros((n,3,224,224))\n",
    "    sunglasses_faces = np.zeros((n,3,224,224))\n",
    "\n",
    "    for i in range(1,n+1):\n",
    "        if i == 47: continue     \n",
    "        path = str('cropped/')\n",
    "            \n",
    "        for j in range(1,6):\n",
    "            image_path = path + 'TD_RGB_E_{}_{}.jpg'.format(i,j)\n",
    "            p = Image.open(image_path)        \n",
    "#             size = p.size\n",
    "#             p = ImageOps.grayscale(p)\n",
    "            p = np.transpose(np.array(p))\n",
    "#             print(np.transpose(p).shape)\n",
    "            \n",
    "            if j == 1: neutral_faces[i-1,:] = p\n",
    "            elif j == 2: smiling_faces[i-1,:] = p\n",
    "            elif j ==3: eyesclosed_faces[i-1,:] = p\n",
    "            elif j == 4: surprised_faces[i-1,:] = p\n",
    "            elif j == 5: sunglasses_faces[i-1,:] = p\n",
    "\n",
    "#     return np.transpose(neutral_faces), np.transpose(smiling_faces), np.transpose(eyesclosed_faces), np.transpose(surprised_faces), np.transpose(sunglasses_faces)\n",
    "    return neutral_faces, smiling_faces, eyesclosed_faces, surprised_faces, sunglasses_faces\n",
    "\n",
    "neutral_faces, smiling_faces, eyesclosed_faces, surprised_faces, sunglasses_faces = get_faces()\n",
    "print(neutral_faces.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(245, 3, 224, 224)\n",
      "<class 'numpy.ndarray'>\n",
      "(217, 3, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "# dataset\n",
    "\n",
    "neutral_faces, smiling_faces, eyesclosed_faces, surprised_faces, sunglasses_faces = get_faces()\n",
    "\n",
    "\n",
    "data = np.vstack((neutral_faces, smiling_faces, eyesclosed_faces, surprised_faces, sunglasses_faces))\n",
    "labels_0 = [0] * len(neutral_faces)\n",
    "labels_1 = [1] * len(neutral_faces)\n",
    "labels_2 = [2] * len(neutral_faces)\n",
    "labels_3 = [3] * len(neutral_faces)\n",
    "labels_4 = [4] * len(neutral_faces)\n",
    "\n",
    "labels = labels_0 + labels_1 + labels_2 + labels_3 + labels_4\n",
    "labels = np.array(labels)\n",
    "idxs = list(range(245))   # will change when do data augmentation\n",
    "np.random.shuffle(idxs)\n",
    "\n",
    "data = data[idxs]\n",
    "labels = labels[idxs]\n",
    "\n",
    "print(type(data))\n",
    "print(data.shape)\n",
    "\n",
    "test_train_split = 0.88889\n",
    "train_data = data[:int(test_train_split * len(idxs))]\n",
    "train_labels = labels[:int(test_train_split * len(idxs))]\n",
    "\n",
    "print(type(train_data))\n",
    "print(train_data.shape)\n",
    "\n",
    "test_data = data[int(test_train_split * len(idxs)):]\n",
    "test_labels = labels[int(test_train_split * len(idxs)):]\n",
    "\n",
    "# train_data = torch.tensor(train_data)\n",
    "# train_labels = torch.tensor(train_labels)\n",
    "# train_data = TensorDataset(train_data, train_labels)\n",
    "\n",
    "# test_data = torch.tensor(test_data)\n",
    "# test_labels = torch.tensor(test_labels)\n",
    "# test_data = TensorDataset(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49, 3, 224, 224)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 25 is out of bounds for axis 1 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-2d4e3abe9fde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Executing test function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneutral_faces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmiling_faces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meyesclosed_faces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msurprised_faces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msunglasses_faces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-2d4e3abe9fde>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(neutral_faces, smiling_faces, eyesclosed_faces, surprised_faces, sunglasses_faces, n)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneutral_faces\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmiling_faces\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 25 is out of bounds for axis 1 with size 3"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def test(neutral_faces, smiling_faces, eyesclosed_faces, surprised_faces, sunglasses_faces, n): \n",
    "# def test(neutral_faces,smiling_faces,eyesclosed_faces, n): \n",
    "    '''\n",
    "    Function to test that I am able to recontruct the original image with after all the reshaping and shaping.\n",
    "    Checking the dimensions and shape as well. \n",
    "    '''\n",
    "    print(neutral_faces.shape)\n",
    "\n",
    "    plt.subplot(1,5,1)\n",
    "    plt.imshow((neutral_faces[:,n]).reshape((224,224)))\n",
    "    plt.subplot(1,5,2)\n",
    "    plt.imshow((smiling_faces[:,n]).reshape((224,224)))\n",
    "    plt.subplot(1,5,3)\n",
    "    plt.imshow((eyesclosed_faces[:,n]).reshape((224,224)))\n",
    "    plt.subplot(1,5,4)\n",
    "    plt.imshow((surprised_faces[:,n]).reshape((224,224)))\n",
    "    plt.subplot(1,5,5)\n",
    "    plt.imshow((sunglasses_faces[:,n]).reshape((224,224)))\n",
    "\n",
    "    # Executing test function        \n",
    "test(neutral_faces, smiling_faces, eyesclosed_faces, surprised_faces, sunglasses_faces, 25)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, targets, transform=None):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.targets[index]\n",
    "        \n",
    "        if self.transform:\n",
    "            x = Image.fromarray(self.data[index].astype(np.uint8).transpose(1,2,0))\n",
    "            x = self.transform(x)\n",
    "#         print(x)\n",
    "        data = {'x': x, 'y': y}\n",
    "        return data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "train_data = MyDataset(train_data, train_labels)\n",
    "# dataloader = DataLoader(dataset, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://vision.ucsd.edu/content/yale-face-database\n",
    "images = []\n",
    "\n",
    "\n",
    "for f in glob.iglob('Tufts/Set1/1/*'):\n",
    "    images.append(np.asarray(Image.open(f)))\n",
    "#     print(f)\n",
    "#     break\n",
    "    \n",
    "images = np.array(images)\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://vision.ucsd.edu/content/yale-face-database\n",
    "images = []\n",
    "\n",
    "for i in range(1,3): \n",
    "    for f in glob.iglob('Tufts/Set1/'+  str(i) + '/*'):\n",
    "        images.append(np.asarray(Image.open(f)))\n",
    "#         print(f)\n",
    "#         break\n",
    "\n",
    "\n",
    "print(images.shape)\n",
    "# images = np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(images[0,:,:,:])\n",
    "# im1 = Image.open('Tufts/Set1/25/TD_RGB_A_4_6.jpg')\n",
    "plt.imshow(images[,:,:,:], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# im1 = Image.open('yalefaces/subject12.glasses')\n",
    "im1 = Image.open('Tufts/Set1/25/TD_RGB_A_4_6.jpg')\n",
    "plt.imshow(np.array(im1), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im1 = Image.open('Tufts/Set1/25/TD_RGB_A_4_7.jpg')\n",
    "plt.imshow(np.array(im1), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im1 = Image.open('Tufts/Set1/25/TD_RGB_A_4_8.jpg')\n",
    "plt.imshow(np.array(im1), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test\n",
    "# train: centerlight, glasses, happy, leftlight, noglasses, normal, rightlight, sad\n",
    "# test: sleepy, surprised, and wink\n",
    "\n",
    "train_imgs = ['centerlight', 'glasses', 'happy', 'leftlight', 'noglasses', 'normal', 'rightlight', 'sad']\n",
    "test_imgs = ['sleepy', 'surprised', 'wink']\n",
    "\n",
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "images = []\n",
    "for f in glob.iglob('yalefaces/*'):\n",
    "    if f.split(\".\", 1)[1] in train_imgs:\n",
    "        train_data.append(np.asarray(Image.open(f)))\n",
    "    else:\n",
    "        test_data.append(np.asarray(Image.open(f)))\n",
    "        \n",
    "train_data = np.array(train_data)\n",
    "test_data = np.array(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5/27 to do next:\n",
    "- need dataloader\n",
    "- figure out if data is already centered (if not, how to do it for PCA)\n",
    "- feature extraction (PCA)\n",
    "- train our ELMAN RNN (link for how to use RNN with Pytorch: https://www.kaggle.com/kanncaa1/recurrent-neural-network-with-pytorch, https://pytorch.org/docs/stable/generated/torch.nn.RNN.html)\n",
    "- find optimal weights\n",
    "- test\n",
    "- try out other types of netral networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference*\n",
    "Any publication using this database must reference to this: \n",
    "- Website: http://tdface.ece.tufts.edu/ and this \n",
    "- Paper: Panetta, Karen, Qianwen Wan, Sos Agaian, Srijith Rajeev, Shreyas Kamath, Rahul Rajendran, Shishir Rao et al. \"A comprehensive database for benchmarking imaging systems.\" IEEE Transactions on Pattern Analysis and Machine Intelligence (2018)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/timesler/guide-to-mtcnn-in-facenet-pytorch\n",
    "mtcnn = MTCNN(image_size=224, margin=20, keep_all=True, post_process=False, device='cuda')\n",
    "\n",
    "# Load a single image and display\n",
    "frame = cv2.imread('TD_RGB_E/1/TD_RGB_E_1.jpg')\n",
    "frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "frame = Image.fromarray(frame)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(frame)\n",
    "plt.axis('off')\n",
    "\n",
    "# Detect face\n",
    "face = mtcnn(frame)\n",
    "face.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# face detection code, save cropped face \n",
    "# https://www.kaggle.com/timesler/guide-to-mtcnn-in-facenet-pytorch\n",
    "IMAGE_SIZE = 224\n",
    "\n",
    "for person in range(1, 51):\n",
    "    if person == 47: continue\n",
    "    for idx in range(1, 6):\n",
    "        # Load a single image and display\n",
    "        frame = cv2.imread(f'TD_RGB_E/{person}/TD_RGB_E_{idx}.jpg')\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = cv2.resize(frame, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "        frame = Image.fromarray(frame)\n",
    "\n",
    "        mtcnn(frame, save_path=f'cropped/TD_RGB_E_{person}_{idx}.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAIN = len(train_data)\n",
    "NUM_TEST = len(test_data)\n",
    "\n",
    "def check_accuracy(loader, model):\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')   \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "\n",
    "\n",
    "def train_model(model, optimizer, epochs=1):\n",
    "    \"\"\"\n",
    "    Train a model lol\n",
    "    \n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    model.train()  # put model to training mode\n",
    "    for e in range(epochs):\n",
    "        for t, data in enumerate(loader_train):\n",
    "#             print(data)\n",
    "            x = data['x'].to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = data['y'].to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "            if t % print_every == 0:\n",
    "                print('Epoch %d, Iteration %d, loss = %.4f' % (e, t, loss.item()))\n",
    "                check_accuracy(loader_val, model)\n",
    "                print()\n",
    "\n",
    "# We set up a Dataset object for each split (train / val / test); Datasets load\n",
    "# training examples one at a time, so we wrap each Dataset in a DataLoader which\n",
    "# iterates through the Dataset and forms minibatches. We divide the CIFAR-100\n",
    "# training set into train and val sets by passing a Sampler object to the\n",
    "# DataLoader telling how it should sample from the underlying Dataset.\n",
    "loader_train = DataLoader(train_data, batch_size=12, sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "\n",
    "# loader_val = DataLoader(cifar100_val, batch_size=batch_size, num_workers=2, \n",
    "#                         sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n",
    "\n",
    "# cifar100_test = dset.CIFAR100('./datasets/cifar100', train=False, download=True, \n",
    "#                             transform=transform_test)\n",
    "loader_test = DataLoader(test_data, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size 32 3 3 3, expected input[12, 32, 112, 112] to have 3 channels, but got 32 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-7ccfbf50707e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint_every\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-6f7f2ee9886c>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, epochs)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/285_project/ECE285_Project/woodnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    338\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    339\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 340\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size 32 3 3 3, expected input[12, 32, 112, 112] to have 3 channels, but got 32 channels instead"
     ]
    }
   ],
   "source": [
    "# define neural network\n",
    "model = WoodNet()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001, weight_decay=0.00001)\n",
    "\n",
    "print_every = 10\n",
    "train_model(model, optimizer, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set accuracy\n",
    "check_accuracy(loader_test, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
